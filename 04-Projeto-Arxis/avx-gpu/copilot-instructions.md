# AVX-GPU - Copilot Instructions

**Projeto**: avx-gpu
**DescriÃ§Ã£o**: Cross-Platform GPU Compute Framework - Surpassing CUDA in Developer Experience
**Status**: v0.1.0 - Foundation Complete, Multi-Backend Expansion
**Filosofia**: Performance + Portability. Rust-first. Zero C++ footprint.

---

## ğŸ¯ REGRAS CRÃTICAS - NUNCA VIOLAR

### 1. Cross-Platform Ã© NÃ£o-NegociÃ¡vel
```rust
// âœ… CORRETO: Backend abstraction
pub trait GpuBackend {
    fn device_info(&self) -> DeviceInfo;
    fn compile_kernel(&self, source: &str, entry: &str) -> Result<Kernel>;
    fn execute_kernel(&self, kernel: &Kernel, args: &[&Buffer]) -> Result<()>;
}

// ImplementaÃ§Ãµes:
impl GpuBackend for WgpuBackend { ... }  // âœ… Windows/Linux/macOS/Web
impl GpuBackend for CudaBackend { ... }  // âœ… NVIDIA only
impl GpuBackend for MetalBackend { ... } // âœ… Apple only
impl GpuBackend for RocmBackend { ... }  // âœ… AMD only

// âŒ ERRADO: Hardcoded CUDA
use cudarc::driver::*; // PROIBIDO sem abstraction!
```

**Motivo**: AVL Platform opera globalmente. Brasil tem mix NVIDIA/AMD. Apple users existem.

### 2. Target 90-110% CUDA Performance
```rust
// Benchmark obrigatÃ³rio em cada PR
#[bench]
fn bench_matmul_1024_vs_cuda(b: &mut Bencher) {
    let device = Device::auto().unwrap();
    let a = device.buffer::<f32>(1024 * 1024).unwrap();
    let b = device.buffer::<f32>(1024 * 1024).unwrap();

    b.iter(|| {
        black_box(device.matmul(&a, &b).unwrap())
    });
}

// Target:
// - AVX-GPU (wgpu): 45ms @ RTX 4090
// - CUDA cuBLAS: 40ms @ RTX 4090
// - Ratio: 112% (acceptable!)
// - AVX-GPU (CUDA backend): 38ms (target 95% cuBLAS)
```

**Performance targets por operaÃ§Ã£o**:
- Vector add: >95% CUDA
- Matrix multiply (GEMM): 85-100% cuBLAS
- FFT: 80-95% cuFFT
- Convolution: 85-100% cuDNN

### 3. Type-Safe GPU Memory
```rust
// âœ… CORRETO: Type-safe buffers
pub struct Buffer<T> {
    inner: Arc<dyn BufferImpl>,
    len: usize,
    _marker: PhantomData<T>,
}

impl<T: GpuType> Buffer<T> {
    pub fn read(&self) -> Result<Vec<T>>;
    pub fn write(&mut self, data: &[T]) -> Result<()>;
    pub fn len(&self) -> usize;
}

// Compile-time type checking
let buf_f32: Buffer<f32> = device.buffer(1024)?;
let buf_i32: Buffer<i32> = device.buffer(1024)?;

device.execute_kernel(&kernel, &[&buf_f32, &buf_i32])?; // âœ… Types checked

// âŒ ERRADO: Type-erased buffers
let buf: Buffer = device.buffer(1024, "f32")?; // Runtime type!
```

### 4. WGSL Como Lingua Franca
```rust
// âœ… CORRETO: WGSL shader (cross-platform)
const VECTOR_ADD: &str = r#"
@group(0) @binding(0) var<storage, read> a: array<f32>;
@group(0) @binding(1) var<storage, read> b: array<f32>;
@group(0) @binding(2) var<storage, read_write> c: array<f32>;

@compute @workgroup_size(256)
fn vector_add(@builtin(global_invocation_id) id: vec3<u32>) {
    let idx = id.x;
    if (idx < arrayLength(&a)) {
        c[idx] = a[idx] + b[idx];
    }
}
"#;

// Futuro: Rust â†’ WGSL compiler
#[gpu_kernel]
fn vector_add(a: &[f32], b: &[f32]) -> Vec<f32> {
    a.iter().zip(b).map(|(x, y)| x + y).collect()
}

// âŒ ERRADO: CUDA-specific code sem abstraction
const CUDA_KERNEL: &str = "__global__ void kernel() { ... }"; // PROIBIDO!
```

---

## ğŸ“ Arquitetura do Projeto

```
avx-gpu/
â”œâ”€â”€ avx-gpu-core/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ lib.rs             # Public API
â”‚   â”‚   â”œâ”€â”€ device.rs          # Device abstraction
â”‚   â”‚   â”œâ”€â”€ buffer.rs          # Type-safe buffers
â”‚   â”‚   â”œâ”€â”€ kernel.rs          # Kernel abstraction
â”‚   â”‚   â”œâ”€â”€ backend.rs         # Backend trait
â”‚   â”‚   â”œâ”€â”€ error.rs           # Error types
â”‚   â”‚   â””â”€â”€ types.rs           # GpuType trait
â”‚   â””â”€â”€ Cargo.toml
â”œâ”€â”€ avx-gpu-backends/
â”‚   â”œâ”€â”€ wgpu/
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ lib.rs         # WgpuBackend impl
â”‚   â”‚   â”‚   â”œâ”€â”€ device.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ buffer.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ kernel.rs
â”‚   â”‚   â”‚   â””â”€â”€ compiler.rs    # WGSL â†’ SPIR-V
â”‚   â”‚   â””â”€â”€ Cargo.toml
â”‚   â”œâ”€â”€ cuda/
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ lib.rs         # CudaBackend impl
â”‚   â”‚   â”‚   â”œâ”€â”€ device.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ buffer.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ kernel.rs
â”‚   â”‚   â”‚   â””â”€â”€ compiler.rs    # WGSL â†’ PTX
â”‚   â”‚   â””â”€â”€ Cargo.toml
â”‚   â”œâ”€â”€ metal/
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ lib.rs         # MetalBackend impl
â”‚   â”‚   â”‚   â”œâ”€â”€ device.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ buffer.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ kernel.rs
â”‚   â”‚   â”‚   â””â”€â”€ compiler.rs    # WGSL â†’ Metal
â”‚   â”‚   â””â”€â”€ Cargo.toml
â”‚   â”œâ”€â”€ rocm/
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ lib.rs         # RocmBackend impl
â”‚   â”‚   â”‚   â”œâ”€â”€ device.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ buffer.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ kernel.rs
â”‚   â”‚   â”‚   â””â”€â”€ compiler.rs    # WGSL â†’ AMDGPU
â”‚   â”‚   â””â”€â”€ Cargo.toml
â”‚   â””â”€â”€ vulkan/
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ lib.rs         # VulkanBackend (via ash)
â”‚       â”‚   â””â”€â”€ ...
â”‚       â””â”€â”€ Cargo.toml
â”œâ”€â”€ avx-gpu-compiler/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ lib.rs
â”‚   â”‚   â”œâ”€â”€ parser.rs          # Rust AST â†’ IR
â”‚   â”‚   â”œâ”€â”€ optimizer.rs       # IR optimizations
â”‚   â”‚   â”œâ”€â”€ codegen/
â”‚   â”‚   â”‚   â”œâ”€â”€ wgsl.rs        # IR â†’ WGSL
â”‚   â”‚   â”‚   â”œâ”€â”€ spirv.rs       # IR â†’ SPIR-V
â”‚   â”‚   â”‚   â”œâ”€â”€ ptx.rs         # IR â†’ PTX (CUDA)
â”‚   â”‚   â”‚   â””â”€â”€ metal.rs       # IR â†’ Metal
â”‚   â”‚   â””â”€â”€ analysis.rs        # Data flow analysis
â”‚   â””â”€â”€ Cargo.toml
â”œâ”€â”€ avx-gpu-runtime/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ lib.rs
â”‚   â”‚   â”œâ”€â”€ scheduler.rs       # Multi-GPU scheduling
â”‚   â”‚   â”œâ”€â”€ memory_pool.rs     # Pooled allocator
â”‚   â”‚   â”œâ”€â”€ stream.rs          # Async execution
â”‚   â”‚   â””â”€â”€ profiler.rs        # Performance profiling
â”‚   â””â”€â”€ Cargo.toml
â”œâ”€â”€ avx-gpu-std/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ lib.rs
â”‚   â”‚   â”œâ”€â”€ linalg/
â”‚   â”‚   â”‚   â”œâ”€â”€ vector.rs      # BLAS Level 1
â”‚   â”‚   â”‚   â”œâ”€â”€ matrix.rs      # BLAS Level 2/3
â”‚   â”‚   â”‚   â”œâ”€â”€ gemm.rs        # Optimized GEMM
â”‚   â”‚   â”‚   â””â”€â”€ svd.rs         # SVD decomposition
â”‚   â”‚   â”œâ”€â”€ signal/
â”‚   â”‚   â”‚   â”œâ”€â”€ fft.rs         # FFT (Cooley-Tukey)
â”‚   â”‚   â”‚   â”œâ”€â”€ convolution.rs
â”‚   â”‚   â”‚   â””â”€â”€ filter.rs
â”‚   â”‚   â”œâ”€â”€ image/
â”‚   â”‚   â”‚   â”œâ”€â”€ resize.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ convolution.rs
â”‚   â”‚   â”‚   â””â”€â”€ transform.rs
â”‚   â”‚   â””â”€â”€ nn/
â”‚   â”‚       â”œâ”€â”€ conv2d.rs      # Convolution layers
â”‚   â”‚       â”œâ”€â”€ linear.rs      # Fully connected
â”‚   â”‚       â””â”€â”€ activation.rs  # ReLU, etc.
â”‚   â””â”€â”€ Cargo.toml
â”œâ”€â”€ avx-gpu-macros/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ lib.rs
â”‚   â”‚   â””â”€â”€ gpu_kernel.rs      # #[gpu_kernel] macro
â”‚   â””â”€â”€ Cargo.toml
â””â”€â”€ examples/
    â”œâ”€â”€ vector_add.rs
    â”œâ”€â”€ matrix_multiply.rs
    â”œâ”€â”€ fft.rs
    â””â”€â”€ image_filter.rs
```

---

## ğŸš€ Roadmap de ImplementaÃ§Ã£o

### Fase 1: Foundation (v0.1.0) âœ… COMPLETO
```rust
// âœ… Core API
pub struct Device {
    backend: Arc<dyn GpuBackend>,
}

impl Device {
    pub fn auto() -> Result<Self> {
        // Try backends in order:
        // 1. CUDA (if NVIDIA GPU)
        // 2. Metal (if Apple Silicon)
        // 3. ROCm (if AMD GPU)
        // 4. wgpu (fallback, works everywhere)
    }

    pub fn from_backend(backend: impl GpuBackend + 'static) -> Self;

    pub fn buffer<T: GpuType>(&self, len: usize) -> Result<Buffer<T>>;
    pub fn buffer_from_slice<T: GpuType>(&self, data: &[T]) -> Result<Buffer<T>>;

    pub fn compile_kernel(&self, source: &str, entry: &str) -> Result<Kernel>;
    pub fn execute_kernel(&self, kernel: &Kernel, args: &[&dyn BufferTrait]) -> Result<()>;
}

// âœ… wgpu backend
pub struct WgpuBackend {
    instance: wgpu::Instance,
    adapter: wgpu::Adapter,
    device: wgpu::Device,
    queue: wgpu::Queue,
}
```

**Deliverables**:
- [x] Core abstractions (Device, Buffer, Kernel)
- [x] wgpu backend (cross-platform)
- [x] Type-safe buffer API
- [x] WGSL kernel compilation
- [x] Examples (vector_add, matmul)
- [x] Benchmarks vs CPU

### Fase 2: Multi-Backend (v0.2.0) - Semanas 1-4
```rust
// TODO: CUDA backend
pub struct CudaBackend {
    context: CudaContext,
    device: CudaDevice,
    stream: CudaStream,
}

impl GpuBackend for CudaBackend {
    fn compile_kernel(&self, source: &str, entry: &str) -> Result<Kernel> {
        // 1. Parse WGSL
        // 2. Convert to CUDA PTX
        // 3. Load with cuModuleLoadData
        // 4. Get kernel function handle

        let module = naga::front::wgsl::parse_str(source)?;
        let ptx = wgsl_to_ptx(&module)?;

        let cuda_module = self.context.load_module(&ptx)?;
        let function = cuda_module.get_function(entry)?;

        Ok(Kernel {
            backend_kernel: Box::new(CudaKernel { function }),
        })
    }

    fn execute_kernel(&self, kernel: &Kernel, args: &[&Buffer]) -> Result<()> {
        // Launch kernel with grid/block config
        let grid_size = (args[0].len() + 255) / 256;
        let block_size = 256;

        unsafe {
            kernel.launch(
                grid_size,
                block_size,
                args.iter().map(|b| b.device_ptr()).collect(),
            )?;
        }

        self.stream.synchronize()?;
        Ok(())
    }
}

// TODO: Metal backend
pub struct MetalBackend {
    device: metal::Device,
    command_queue: metal::CommandQueue,
}

impl GpuBackend for MetalBackend {
    fn compile_kernel(&self, source: &str, entry: &str) -> Result<Kernel> {
        // WGSL â†’ Metal Shading Language
        let module = naga::front::wgsl::parse_str(source)?;
        let msl = wgsl_to_metal(&module)?;

        let library = self.device.new_library_with_source(&msl, &metal::CompileOptions::new())?;
        let function = library.get_function(entry, None)?;

        Ok(Kernel {
            backend_kernel: Box::new(MetalKernel { function }),
        })
    }
}

// TODO: ROCm backend (HIP)
pub struct RocmBackend {
    device: hip_sys::Device,
    context: hip_sys::Context,
}

impl GpuBackend for RocmBackend {
    fn compile_kernel(&self, source: &str, entry: &str) -> Result<Kernel> {
        // WGSL â†’ AMDGPU assembly
        let module = naga::front::wgsl::parse_str(source)?;
        let asm = wgsl_to_amdgpu(&module)?;

        // Compile with ROCm compiler
        let code_object = hip_compile(&asm)?;

        Ok(Kernel {
            backend_kernel: Box::new(RocmKernel { code_object }),
        })
    }
}
```

**Backends a implementar**:
1. CUDA (NVIDIA) âœ… Priority 1
2. Metal (Apple) âœ… Priority 2
3. ROCm (AMD) âœ… Priority 3
4. Vulkan (via ash, fallback) â³ Priority 4

### Fase 3: Kernel Compiler (v0.3.0) - Semanas 5-8
```rust
// TODO: #[gpu_kernel] macro
#[gpu_kernel]
fn vector_add(a: &[f32], b: &[f32]) -> Vec<f32> {
    a.iter().zip(b).map(|(x, y)| x + y).collect()
}

// Expands to:
mod __gpu_vector_add {
    pub const WGSL_SOURCE: &str = r#"
        @group(0) @binding(0) var<storage, read> a: array<f32>;
        @group(0) @binding(1) var<storage, read> b: array<f32>;
        @group(0) @binding(2) var<storage, read_write> result: array<f32>;

        @compute @workgroup_size(256)
        fn vector_add(@builtin(global_invocation_id) id: vec3<u32>) {
            let idx = id.x;
            if (idx < arrayLength(&a)) {
                result[idx] = a[idx] + b[idx];
            }
        }
    "#;

    pub fn run(device: &Device, a: &[f32], b: &[f32]) -> Result<Vec<f32>> {
        let buf_a = device.buffer_from_slice(a)?;
        let buf_b = device.buffer_from_slice(b)?;
        let mut buf_result = device.buffer::<f32>(a.len())?;

        let kernel = device.compile_kernel(WGSL_SOURCE, "vector_add")?;
        device.execute_kernel(&kernel, &[&buf_a, &buf_b, &buf_result])?;

        buf_result.read()
    }
}

pub use __gpu_vector_add::run as vector_add;

// Compiler pipeline:
// 1. Parse Rust AST (syn)
// 2. Convert to intermediate IR
// 3. Optimize IR (dead code, constant folding)
// 4. Generate WGSL
// 5. Embed in Rust code
```

### Fase 4: GPU Standard Library (v0.4.0) - Semanas 9-12
```rust
// TODO: GPU BLAS (Basic Linear Algebra Subprograms)
pub mod linalg {
    // Level 1: Vector operations
    pub fn dot(a: &Buffer<f32>, b: &Buffer<f32>) -> Result<f32>;
    pub fn axpy(alpha: f32, x: &Buffer<f32>, y: &mut Buffer<f32>) -> Result<()>; // y = alpha*x + y
    pub fn norm(x: &Buffer<f32>) -> Result<f32>;

    // Level 2: Matrix-vector
    pub fn gemv(
        alpha: f32,
        a: &Buffer<f32>,  // m x n matrix
        x: &Buffer<f32>,  // n vector
        beta: f32,
        y: &mut Buffer<f32>, // m vector
    ) -> Result<()>; // y = alpha*A*x + beta*y

    // Level 3: Matrix-matrix
    pub fn gemm(
        alpha: f32,
        a: &Buffer<f32>,  // m x k
        b: &Buffer<f32>,  // k x n
        beta: f32,
        c: &mut Buffer<f32>, // m x n
    ) -> Result<()>; // C = alpha*A*B + beta*C

    // Advanced
    pub fn svd(a: &Buffer<f32>) -> Result<SVDResult>;
    pub fn qr(a: &Buffer<f32>) -> Result<QRResult>;
}

// TODO: GPU Signal Processing
pub mod signal {
    pub fn fft(signal: &Buffer<Complex<f32>>) -> Result<Buffer<Complex<f32>>>;
    pub fn ifft(spectrum: &Buffer<Complex<f32>>) -> Result<Buffer<Complex<f32>>>;
    pub fn convolve(a: &Buffer<f32>, b: &Buffer<f32>) -> Result<Buffer<f32>>;
    pub fn correlate(a: &Buffer<f32>, b: &Buffer<f32>) -> Result<Buffer<f32>>;
}

// TODO: GPU Image Processing
pub mod image {
    pub fn resize(
        input: &Buffer<u8>,
        input_size: (usize, usize),
        output_size: (usize, usize),
    ) -> Result<Buffer<u8>>;

    pub fn gaussian_blur(
        input: &Buffer<u8>,
        size: (usize, usize),
        sigma: f32,
    ) -> Result<Buffer<u8>>;

    pub fn sobel_filter(
        input: &Buffer<u8>,
        size: (usize, usize),
    ) -> Result<Buffer<u8>>;
}
```

### Fase 5: Advanced Features (v0.5.0) - Semanas 13-16
```rust
// TODO: Multi-GPU support
pub struct MultiGpuDevice {
    devices: Vec<Device>,
    scheduler: Scheduler,
}

impl MultiGpuDevice {
    pub fn all_gpus() -> Result<Self>;

    pub fn split_buffer<T: GpuType>(&self, data: &[T]) -> Result<Vec<Buffer<T>>> {
        // Distribute data across GPUs
        // Use NCCL/RCCL for inter-GPU communication
    }

    pub fn parallel_execute<F>(&self, f: F) -> Result<()>
    where
        F: Fn(&Device) -> Result<()> + Send + Sync;
}

// TODO: Async execution
pub struct Stream {
    backend_stream: Box<dyn StreamImpl>,
}

impl Stream {
    pub fn execute_async(&self, kernel: &Kernel, args: &[&Buffer]) -> Result<Event>;
    pub fn synchronize(&self) -> Result<()>;
}

pub struct Event {
    backend_event: Box<dyn EventImpl>,
}

impl Event {
    pub fn wait(&self) -> Result<()>;
    pub fn is_complete(&self) -> bool;
}

// TODO: Memory pool
pub struct MemoryPool {
    free_blocks: Vec<(usize, *mut u8)>,
    used_blocks: HashMap<*mut u8, usize>,
}

impl MemoryPool {
    pub fn allocate(&mut self, size: usize) -> Result<*mut u8>;
    pub fn deallocate(&mut self, ptr: *mut u8);
    pub fn reset(&mut self); // Free all without deallocation
}

// TODO: Auto-tuning
pub struct AutoTuner {
    cache: HashMap<String, TuneParams>,
}

impl AutoTuner {
    pub fn tune_kernel(&mut self, kernel: &Kernel) -> Result<TuneParams> {
        // Try different block sizes, shared memory configs
        // Cache best configuration
    }
}
```

---

## ğŸ§ª Testes ObrigatÃ³rios

### 1. Cross-Backend Compatibility
```rust
#[test]
fn test_vector_add_all_backends() {
    let backends = vec![
        Device::wgpu(),
        Device::cuda().ok(),
        Device::metal().ok(),
        Device::rocm().ok(),
    ];

    for backend in backends.into_iter().flatten() {
        let a = backend.buffer_from_slice(&[1.0f32, 2.0, 3.0, 4.0]).unwrap();
        let b = backend.buffer_from_slice(&[5.0f32, 6.0, 7.0, 8.0]).unwrap();
        let mut c = backend.buffer::<f32>(4).unwrap();

        let kernel = backend.compile_kernel(VECTOR_ADD, "vector_add").unwrap();
        backend.execute_kernel(&kernel, &[&a, &b, &c]).unwrap();

        let result = c.read().unwrap();
        assert_eq!(result, vec![6.0, 8.0, 10.0, 12.0]);
    }
}
```

### 2. Performance vs CUDA
```rust
#[bench]
fn bench_matmul_1024_avx_gpu(b: &mut Bencher) {
    let device = Device::auto().unwrap();
    let a = device.buffer::<f32>(1024 * 1024).unwrap();
    let b = device.buffer::<f32>(1024 * 1024).unwrap();

    b.iter(|| {
        black_box(linalg::gemm(1.0, &a, &b, 0.0, &mut c).unwrap())
    });
}

#[bench]
fn bench_matmul_1024_cublas(b: &mut Bencher) {
    // Compare against cuBLAS
    let handle = cublas::CublasHandle::new().unwrap();
    // ...
}

// Target: AVX-GPU >= 90% cuBLAS performance
```

### 3. Memory Safety
```rust
#[test]
fn test_buffer_type_safety() {
    let device = Device::auto().unwrap();

    let buf_f32: Buffer<f32> = device.buffer(1024).unwrap();
    let buf_i32: Buffer<i32> = device.buffer(1024).unwrap();

    // This should compile
    let _: Vec<f32> = buf_f32.read().unwrap();

    // This should NOT compile (type mismatch)
    // let _: Vec<i32> = buf_f32.read().unwrap(); // âŒ
}
```

---

## ğŸ“Š API PÃºblica

### Core API
```rust
pub struct Device {
    backend: Arc<dyn GpuBackend>,
}

impl Device {
    // Device selection
    pub fn auto() -> Result<Self>;
    pub fn wgpu() -> Result<Self>;
    pub fn cuda() -> Result<Self>;
    pub fn metal() -> Result<Self>;
    pub fn rocm() -> Result<Self>;
    pub fn from_backend(backend: impl GpuBackend + 'static) -> Self;

    // Device info
    pub fn name(&self) -> &str;
    pub fn vendor(&self) -> Vendor;
    pub fn compute_units(&self) -> usize;
    pub fn memory_size(&self) -> usize;

    // Memory management
    pub fn buffer<T: GpuType>(&self, len: usize) -> Result<Buffer<T>>;
    pub fn buffer_from_slice<T: GpuType>(&self, data: &[T]) -> Result<Buffer<T>>;

    // Kernel execution
    pub fn compile_kernel(&self, source: &str, entry: &str) -> Result<Kernel>;
    pub fn execute_kernel(&self, kernel: &Kernel, args: &[&dyn BufferTrait]) -> Result<()>;
}

pub struct Buffer<T> {
    inner: Arc<dyn BufferImpl>,
    len: usize,
    _marker: PhantomData<T>,
}

impl<T: GpuType> Buffer<T> {
    pub fn len(&self) -> usize;
    pub fn read(&self) -> Result<Vec<T>>;
    pub fn write(&mut self, data: &[T]) -> Result<()>;
    pub fn copy_from(&mut self, src: &Buffer<T>) -> Result<()>;
}

pub struct Kernel {
    backend_kernel: Box<dyn KernelImpl>,
}

pub trait GpuType: Copy + Send + Sync + 'static {
    fn wgsl_type() -> &'static str;
}

impl GpuType for f32 {
    fn wgsl_type() -> &'static str { "f32" }
}
impl GpuType for i32 {
    fn wgsl_type() -> &'static str { "i32" }
}
// ... outros tipos
```

---

## âš ï¸ Erros Comuns a Evitar

### 1. Backend-Specific Code
```rust
// âŒ ERRADO: Expor detalhes do backend
pub fn execute_cuda_kernel(ptx: &str) { ... }

// âœ… CORRETO: Backend-agnostic API
pub fn execute_kernel(kernel: &Kernel) { ... }
```

### 2. Synchronous API Blocking
```rust
// âŒ ERRADO: Block main thread
let result = buffer.read()?; // Blocks!

// âœ… CORRETO: Async where possible
let result = buffer.read_async().await?;
```

### 3. Memory Leaks
```rust
// âŒ ERRADO: No cleanup
fn process() {
    let buf = device.buffer::<f32>(1_000_000)?; // Leak!
    // Esqueceu de dropar
}

// âœ… CORRETO: RAII pattern
fn process() {
    let buf = device.buffer::<f32>(1_000_000)?;
    // ... use buf
    // Automatic drop at end of scope
}
```

---

## ğŸ† Checklist de Qualidade

Antes de fazer PR:

- [ ] **Cross-Platform**: Funciona em wgpu, CUDA, Metal, ROCm
- [ ] **Performance**: â‰¥90% CUDA para operaÃ§Ã£o testada
- [ ] **Type Safety**: Buffers type-safe em compile-time
- [ ] **Zero Unsafe**: Minimizar unsafe Rust
- [ ] **Docs**: Cada funÃ§Ã£o pÃºblica documentada
- [ ] **Tests**: Unit tests + cross-backend tests
- [ ] **Benchmarks**: vs CUDA/cuBLAS/cuDNN
- [ ] **Examples**: CÃ³digo funcional para usuÃ¡rios

---

## ğŸš€ Como ComeÃ§ar

### Setup
```bash
cd arxis/avx-gpu
cargo build --all
cargo test --all
```

### Exemplos
```bash
# Vector add (cross-platform)
cargo run --example vector_add

# Matrix multiply (optimized)
cargo run --example matrix_multiply

# FFT
cargo run --example fft
```

### Benchmarks
```bash
# Internal benchmarks
cargo bench --workspace

# vs CUDA (se disponÃ­vel)
cargo bench --bench vs_cuda
```

---

**Lembre-se**: Cross-platform Ã© nÃ£o-negociÃ¡vel. Performance > 90% CUDA. Type-safety em compile-time.

**AVX-GPU** - GPU Computing for Everyone ğŸš€
