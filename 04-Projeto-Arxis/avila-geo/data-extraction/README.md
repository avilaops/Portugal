# ğŸ•·ï¸ Data Extraction & Web Scraping Agent

**Enterprise-grade web scraping toolkit** for automated data collection, specialized in Portugal & LATAM market intelligence.

## ğŸ¯ Features

- âœ… **Robust Scraping**: Rate limiting, retries, proxy rotation
- âœ… **Multi-Source**: LinkedIn, Google Maps, ITJobs, Idealista, and more
- âœ… **AvilaDB Integration**: Automatic storage with deduplication
- âœ… **Anti-Detection**: User-agent rotation, request delays, robots.txt compliance
- âœ… **JavaScript Support**: Headless Chrome for SPA scraping
- âœ… **Data Quality**: Validation, normalization, cleaning
- âœ… **Monitoring**: Real-time metrics and alerting
- âœ… **Ethical**: GDPR-compliant, respects ToS

## ğŸš€ Quick Start

### Installation

```bash
# Install Rust (if not already)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# Clone repository
git clone https://github.com/avelan/data-extraction-agent
cd data-extraction-agent

# Build project
cargo build --release
```

### Basic Usage

```rust
use scraper_core::{ScraperEngine, AntiDetectionStrategy, extractors::LinkedInCompanyExtractor};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize scraper
    let scraper = ScraperEngine::builder()
        .with_rate_limit(10) // 10 requests per second
        .with_anti_detection(AntiDetectionStrategy::default())
        .build()?;

    // Scrape LinkedIn company
    let html = scraper.scrape_url("https://www.linkedin.com/company/example").await?;

    // Extract data
    let extractor = LinkedInCompanyExtractor::new();
    let company = extractor.extract(&html)?;

    println!("Company: {:?}", company);

    Ok(())
}
```

## ğŸ“¦ Project Structure

```
data-extraction/
â”œâ”€â”€ scraper-core/          # Core scraping engine
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ engine.rs      # Main scraper engine
â”‚   â”‚   â”œâ”€â”€ extractors/    # Data extractors
â”‚   â”‚   â”œâ”€â”€ storage/       # AvilaDB integration
â”‚   â”‚   â”œâ”€â”€ anti_detect/   # Anti-detection strategies
â”‚   â”‚   â””â”€â”€ monitoring/    # Metrics & quality control
â”œâ”€â”€ scraper-cli/           # CLI tool
â”‚   â””â”€â”€ src/
â”‚       â””â”€â”€ main.rs
â”œâ”€â”€ examples/              # Usage examples
â”‚   â”œâ”€â”€ linkedin_scraper.rs
â”‚   â”œâ”€â”€ itjobs_scraper.rs
â”‚   â””â”€â”€ idealista_scraper.rs
â””â”€â”€ config/
    â””â”€â”€ default.toml       # Configuration
```

## ğŸ”§ CLI Usage

```bash
# Scrape company from LinkedIn
cargo run --bin scraper-cli -- scrape linkedin --company "example-company"

# Scrape jobs from ITJobs Portugal
cargo run --bin scraper-cli -- scrape itjobs --keyword "rust developer" --location "Lisboa"

# Scrape real estate from Idealista
cargo run --bin scraper-cli -- scrape idealista --city "Porto" --type "apartment"

# View scraping statistics
cargo run --bin scraper-cli -- stats

# Export scraped data
cargo run --bin scraper-cli -- export --format json --output data.json
```

## ğŸŒ Supported Sources

### Portugal-Specific
- **ITJobs.pt** - Tech job listings
- **Idealista** - Real estate listings
- **Racius** - Company information
- **INE Portugal** - Statistics
- **Pordata** - Comprehensive data

### International
- **LinkedIn** - Company profiles, job postings
- **Google Maps** - Business listings, reviews
- **Crunchbase** - Funding data
- **GitHub** - Open source activity

## ğŸ“Š AvilaDB Integration

Store scraped data efficiently with automatic deduplication:

```rust
use scraper_core::storage::ScrapedDataManager;
use aviladb::AvilaClient;

let client = AvilaClient::connect("http://localhost:8000").await?;
let db = client.database("market_intelligence").await?;

let manager = ScrapedDataManager::new(db);

// Store with automatic deduplication
manager.store_company(company_data).await?;
```

## âš–ï¸ Ethical Guidelines

This toolkit enforces ethical scraping:

1. âœ… **Respects robots.txt** automatically
2. âœ… **Rate limiting** prevents server overload
3. âœ… **User-agent identification** for transparency
4. âœ… **GDPR compliance** for personal data
5. âœ… **ToS compliance** checking

## ğŸ”’ Configuration

Create `config/local.toml`:

```toml
[scraper]
rate_limit_per_second = 10
max_concurrent_requests = 5
request_timeout_seconds = 30
max_retries = 3

[proxy]
enabled = true
rotation = "round_robin"
proxies = [
    "http://proxy1.example.com:8080",
    "http://proxy2.example.com:8080"
]

[aviladb]
connection_string = "http://localhost:8000"
database = "market_intelligence"
collection = "companies"

[anti_detection]
randomize_delays = true
min_delay_ms = 500
max_delay_ms = 2000
rotate_user_agents = true
```

## ğŸ“ˆ Monitoring

Real-time metrics dashboard:

```rust
let monitor = scraper.get_monitor();

println!("URLs scraped: {}", monitor.urls_scraped());
println!("Success rate: {:.2}%", monitor.success_rate() * 100.0);
println!("Avg response time: {:.2}ms", monitor.avg_response_time_ms());
println!("Data quality score: {:.2}", monitor.data_quality_score());
```

## ğŸ§ª Testing

```bash
# Run all tests
cargo test

# Run specific test
cargo test test_linkedin_extractor

# Run with logging
RUST_LOG=debug cargo test
```

## ğŸ“ License

MIT License - See LICENSE file for details

## ğŸ¤ Contributing

Contributions welcome! Please read CONTRIBUTING.md first.

---

Built with â¤ï¸ by the Avelan Team for Portugal's digital transformation ğŸ‡µğŸ‡¹
