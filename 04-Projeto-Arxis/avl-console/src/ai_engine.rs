//! Local AI Engine abstraction for AVL Console
//!
//! This module introduces an internal AI backend architecture that allows
//! the console to run without depending on external SaaS LLM providers.
//! For now we provide a lightweight, deterministic stub implementation
//! (`LocalAIDummyBackend`) that can be incrementally replaced by a real
//! inference pipeline (e.g. using candle, ggml, llama.cpp FFI, or a custom
//! Rust transformer implementation).
//!
//! Design Goals:
//! - Pluggable backends (pattern based vs local model vs remote API)
//! - Non-blocking async generation interface
//! - Streaming support readiness (returning a `Stream` of tokens in future)
//! - Deterministic fallback when model not loaded
//!
//! Roadmap for real local model integration:
//! 1. Weight loader for GGUF / safetensors
//! 2. Tokenizer integration (existing avila-tokenizer crate)
//! 3. Quantized matrix ops via avx-gpu / avila-linalg crates
//! 4. KV cache management per session
//! 5. Streaming SSE endpoint
//! 6. Persistence of fine-tune deltas

use std::sync::Arc;
use futures::stream::{self, Stream};

/// Enumeration of backend kinds the assistant can use.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum AIBackendKind {
    /// Existing pattern matching heuristics only.
    Pattern,
    /// Local LLM model (stub for now).
    Local,
}

impl Default for AIBackendKind {
    fn default() -> Self { Self::Pattern }
}

/// Result of an AI generation call.
#[derive(Debug, Clone)]
pub struct AIResult {
    pub text: String,
    pub explanation: Option<String>,
    pub tips: Option<Vec<String>>,
    pub sql: Option<String>,
}

/// Trait every backend must implement.
pub trait AIBackend: Send + Sync {
    fn generate(&self, prompt: &str) -> AIResult;
    /// Streaming generation (token or chunk). Default: single chunk from `generate`.
    fn generate_stream(&self, prompt: &str) -> Box<dyn Stream<Item = String> + Send + Unpin> {
        let result = self.generate(prompt);
        Box::new(stream::iter(vec![result.text]))
    }
}

/// Dummy local backend. Deterministic and fast.
pub struct LocalAIDummyBackend;

impl LocalAIDummyBackend {
    pub fn new() -> Arc<Self> { Arc::new(Self) }
}

impl AIBackend for LocalAIDummyBackend {
    fn generate(&self, prompt: &str) -> AIResult {
        // Very naive heuristic: produce pseudo-SQL if words like SELECT / FROM appear
        let lower = prompt.to_lowercase();
        if lower.contains("schema") {
            return AIResult {
                text: "ðŸ“˜ Local AI: Posso ajudar descrevendo seu schema. Liste tabelas ou campos especÃ­ficos para detalhes.".to_string(),
                explanation: Some("Este Ã© um backend local stub. Para descriÃ§Ã£o de schema real iremos consultar metadados do AvilaDB.".to_string()),
                tips: Some(vec![
                    "Use 'listar tabelas' ou 'detalhar tabela <nome>'".to_string(),
                    "Integre metadados em cache para baixa latÃªncia".to_string(),
                ]),
                sql: None,
            };
        }
        if lower.contains("crie") && lower.contains("tabela") {
            return AIResult {
                text: "ðŸ› ï¸ Local AI: Gerando proposta de DDL para a tabela solicitada.".to_string(),
                explanation: Some("GeraÃ§Ã£o de DDL bÃ¡sica baseada em palavras-chave. Ajuste tipos conforme necessidade.".to_string()),
                tips: Some(vec!["Adicione Ã­ndices para colunas de filtro".to_string(), "Evite muitos campos NULL".to_string()]),
                sql: Some("CREATE TABLE exemplo (
    id UUID PRIMARY KEY,
    nome TEXT NOT NULL,
    criado_em TIMESTAMP DEFAULT NOW()
);".to_string()),
            };
        }
        AIResult {
            text: "ðŸ¤– Local AI stub: modelo local ainda nÃ£o treinado para esta solicitaÃ§Ã£o. ForneÃ§a mais contexto ou habilite o backend padrÃ£o (pattern).".to_string(),
            explanation: Some("Backend local estÃ¡ em modo placeholder. PrÃ³ximas versÃµes incluirÃ£o carregamento de pesos e geraÃ§Ã£o token por token.".to_string()),
            tips: Some(vec![
                "Defina objetivo claro: ex. 'Gerar SQL para vendas por mÃªs'".to_string(),
                "Inclua entidades e filtros explÃ­citos".to_string(),
            ]),
            sql: None,
        }
    }
    fn generate_stream(&self, prompt: &str) -> Box<dyn Stream<Item = String> + Send + Unpin> {
        // Split deterministic output into whitespace tokens for demonstration.
        let text = self.generate(prompt).text;
        let parts: Vec<String> = text.split_whitespace().map(|s| s.to_string()).collect();
        Box::new(stream::iter(parts))
    }
}

/// Pattern backend (wraps existing heuristic path). Provided here to unify under trait.
pub struct PatternBackend;

impl PatternBackend {
    pub fn new() -> Arc<Self> { Arc::new(Self) }
}

impl AIBackend for PatternBackend {
    fn generate(&self, prompt: &str) -> AIResult {
        // Return plain text; actual SQL generation is handled higher up (ai_assistant.rs).
        AIResult { text: prompt.to_string(), explanation: None, tips: None, sql: None }
    }
    fn generate_stream(&self, prompt: &str) -> Box<dyn Stream<Item = String> + Send + Unpin> {
        let tokens: Vec<String> = prompt.split_whitespace().map(|s| s.to_string()).collect();
        Box::new(stream::iter(tokens))
    }
}

/// Resolve backend based on environment or provided kind.
pub fn resolve_backend(kind: AIBackendKind) -> Arc<dyn AIBackend> {
    match kind {
        AIBackendKind::Pattern => PatternBackend::new(),
        AIBackendKind::Local => LocalAIDummyBackend::new(),
    }
}
