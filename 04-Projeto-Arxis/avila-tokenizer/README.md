# ğŸ¯ Prompt Completo: avila-tokenizers - O Tokenizer Mais Completo do Ecossistema Rust

## MissÃ£o
Criar **avila-tokenizers**: a biblioteca de tokenizaÃ§Ã£o mais completa e rÃ¡pida em Rust, superando Hugging Face Tokenizers, tiktoken-rs, e sentencepiece. Deve ser 100% nativa Rust, zero dependÃªncias Python, e otimizada para portuguÃªs brasileiro, com suporte universal para todos os modelos LLM modernos.

---

## ğŸ“¦ Estrutura do Projeto

```
avila-tokenizers/
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib.rs
â”‚   â”œâ”€â”€ algorithms/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ bpe.rs          # Byte-Pair Encoding (GPT-2, GPT-3, GPT-4)
â”‚   â”‚   â”œâ”€â”€ wordpiece.rs    # WordPiece (BERT, DistilBERT)
â”‚   â”‚   â”œâ”€â”€ unigram.rs      # Unigram (SentencePiece, T5, XLNet)
â”‚   â”‚   â”œâ”€â”€ char.rs         # Character-level (ByT5)
â”‚   â”‚   â””â”€â”€ sentencepiece.rs # SentencePiece protocol
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ gpt2.rs         # GPT-2/3/4 tokenizer
â”‚   â”‚   â”œâ”€â”€ gpt4.rs         # GPT-4 (cl100k_base)
â”‚   â”‚   â”œâ”€â”€ bert.rs         # BERT family
â”‚   â”‚   â”œâ”€â”€ llama.rs        # Llama 2/3
â”‚   â”‚   â”œâ”€â”€ claude.rs       # Claude (Anthropic)
â”‚   â”‚   â”œâ”€â”€ mistral.rs      # Mistral 7B
â”‚   â”‚   â””â”€â”€ multilingual.rs # mBERT, XLM-R
â”‚   â”œâ”€â”€ normalizers/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ nfc.rs          # Unicode NFC normalization
â”‚   â”‚   â”œâ”€â”€ nfkc.rs         # Unicode NFKC
â”‚   â”‚   â”œâ”€â”€ lowercase.rs    # Lowercasing
â”‚   â”‚   â”œâ”€â”€ strip.rs        # Strip accents, whitespace
â”‚   â”‚   â””â”€â”€ replace.rs      # Regex-based replacement
â”‚   â”œâ”€â”€ pre_tokenizers/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ whitespace.rs   # Whitespace splitting
â”‚   â”‚   â”œâ”€â”€ byte_level.rs   # GPT-2 byte-level
â”‚   â”‚   â”œâ”€â”€ metaspace.rs    # SentencePiece metaspace
â”‚   â”‚   â”œâ”€â”€ punctuation.rs  # Split on punctuation
â”‚   â”‚   â””â”€â”€ digits.rs       # Split digits
â”‚   â”œâ”€â”€ post_processors/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ bert.rs         # [CLS] + tokens + [SEP]
â”‚   â”‚   â”œâ”€â”€ roberta.rs      # <s> + tokens + </s>
â”‚   â”‚   â””â”€â”€ template.rs     # Custom templates
â”‚   â”œâ”€â”€ decoders/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ byte_level.rs   # GPT-2 byte decoder
â”‚   â”‚   â”œâ”€â”€ wordpiece.rs    # BERT ## removal
â”‚   â”‚   â”œâ”€â”€ metaspace.rs    # _ -> space
â”‚   â”‚   â””â”€â”€ strip.rs        # Remove special tokens
â”‚   â”œâ”€â”€ vocab/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ trie.rs         # Trie data structure
â”‚   â”‚   â”œâ”€â”€ hashmap.rs      # Fast token lookup
â”‚   â”‚   â””â”€â”€ loader.rs       # Load vocab from JSON/txt
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ regex.rs        # Regex patterns
â”‚   â”‚   â”œâ”€â”€ unicode.rs      # Unicode utilities
â”‚   â”‚   â””â”€â”€ cache.rs        # LRU cache for speed
â”‚   â””â”€â”€ error.rs
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ gpt2_tokenize.rs
â”‚   â”œâ”€â”€ bert_tokenize.rs
â”‚   â”œâ”€â”€ llama_tokenize.rs
â”‚   â”œâ”€â”€ train_bpe.rs
â”‚   â”œâ”€â”€ portuguese_example.rs
â”‚   â””â”€â”€ batch_processing.rs
â”œâ”€â”€ benches/
â”‚   â”œâ”€â”€ tokenize_bench.rs
â”‚   â””â”€â”€ compare_hf.rs
â””â”€â”€ tests/
    â”œâ”€â”€ gpt2_tests.rs
    â”œâ”€â”€ bert_tests.rs
    â”œâ”€â”€ llama_tests.rs
    â”œâ”€â”€ unicode_tests.rs
    â””â”€â”€ compatibility_tests.rs
```

---

## ğŸ¯ Requisitos Funcionais

### 1. Algoritmos Fundamentais

#### **BPE (Byte-Pair Encoding)**
```rust
pub struct BPE {
    vocab: HashMap<String, u32>,
    merges: Vec<(String, String)>,
    cache: LruCache<String, Vec<String>>,
}

impl BPE {
    pub fn new(vocab: HashMap<String, u32>, merges: Vec<(String, String)>) -> Self;
    pub fn encode(&self, text: &str) -> Vec<u32>;
    pub fn decode(&self, ids: &[u32]) -> String;
    pub fn train(corpus: &[&str], vocab_size: usize) -> Self;
}
```

**CaracterÃ­sticas:**
- Greedy merging de pares mais frequentes
- Cache LRU para ~10x speedup
- Suporte para byte-level BPE (GPT-2 style)
- PreservaÃ§Ã£o de espaÃ§os em branco

#### **WordPiece (BERT)**
```rust
pub struct WordPiece {
    vocab: HashMap<String, u32>,
    unk_token: String,
    max_input_chars: usize,
}

impl WordPiece {
    pub fn tokenize(&self, text: &str) -> Vec<String>;
    pub fn encode(&self, text: &str) -> Vec<u32>;
    // Longest-match-first strategy
    // ## prefix for subwords
}
```

#### **Unigram (SentencePiece)**
```rust
pub struct Unigram {
    pieces: Vec<(String, f64)>, // token -> log probability
}

impl Unigram {
    pub fn tokenize_with_scores(&self, text: &str) -> Vec<(String, f64)>;
    pub fn train_em(corpus: &[&str], vocab_size: usize, iterations: usize) -> Self;
    // EM algorithm for training
    // Viterbi decoding for tokenization
}
```

---

### 2. Modelos PrÃ©-configurados

#### **GPT-2/3/4**
```rust
pub struct GPT2Tokenizer {
    bpe: BPE,
    encoder: HashMap<String, u32>,
    decoder: HashMap<u32, String>,
    byte_encoder: HashMap<u8, char>,
    byte_decoder: HashMap<char, u8>,
}

impl GPT2Tokenizer {
    pub fn from_pretrained(model: &str) -> Result<Self>; // "gpt2", "gpt2-medium", etc.
    pub fn encode(&self, text: &str) -> Vec<u32>;
    pub fn decode(&self, ids: &[u32]) -> String;
    pub fn encode_batch(&self, texts: &[&str]) -> Vec<Vec<u32>>;
}
```

**VocabulÃ¡rios:**
- GPT-2: 50,257 tokens (r50k_base)
- GPT-3: 50,257 tokens
- GPT-4: 100,256 tokens (cl100k_base)

#### **BERT**
```rust
pub struct BertTokenizer {
    wordpiece: WordPiece,
    normalizer: Box<dyn Normalizer>,
    pre_tokenizer: WhitespaceSplit,
}

impl BertTokenizer {
    pub fn from_pretrained(model: &str) -> Result<Self>; // "bert-base-uncased", etc.
    pub fn encode_with_special(&self, text: &str) -> Vec<u32>; // [CLS] + tokens + [SEP]
    pub fn encode_pair(&self, text_a: &str, text_b: &str) -> Vec<u32>;
}
```

**Special tokens:**
- `[CLS]`: 101
- `[SEP]`: 102
- `[PAD]`: 0
- `[UNK]`: 100
- `[MASK]`: 103

#### **Llama 2/3**
```rust
pub struct LlamaTokenizer {
    sentencepiece: Unigram,
    vocab_size: usize,
}

impl LlamaTokenizer {
    pub fn from_pretrained(model: &str) -> Result<Self>; // "llama-2-7b", "llama-3-70b"
    pub fn encode(&self, text: &str, add_bos: bool, add_eos: bool) -> Vec<u32>;
    // BOS: <s> (1), EOS: </s> (2)
}
```

**VocabulÃ¡rios:**
- Llama 2: 32,000 tokens
- Llama 3: 128,256 tokens (expandido!)

#### **Claude (Anthropic)**
```rust
pub struct ClaudeTokenizer {
    bpe: BPE,
    vocab_size: usize,
}

impl ClaudeTokenizer {
    pub fn from_pretrained(model: &str) -> Result<Self>; // "claude-2", "claude-3"
    // Similar to GPT-4 cl100k_base but with custom vocab
}
```

---

### 3. NormalizaÃ§Ã£o

```rust
pub trait Normalizer: Send + Sync {
    fn normalize(&self, text: &str) -> String;
}

// NFC: Canonical composition (Ã© -> Ã©)
pub struct NFC;

// NFKC: Compatibility composition (ï¬ -> fi)
pub struct NFKC;

// Lowercase
pub struct Lowercase;

// Strip accents: Ã© -> e, Ã§ -> c
pub struct StripAccents;

// Chain normalizers
pub struct Sequence {
    normalizers: Vec<Box<dyn Normalizer>>,
}
```

**Exemplo:**
```rust
let normalizer = Sequence::new(vec![
    Box::new(NFKC),
    Box::new(StripAccents),
    Box::new(Lowercase),
]);
let normalized = normalizer.normalize("OlÃ¡, JosÃ©!");
// Output: "ola, jose!"
```

---

### 4. Pre-TokenizaÃ§Ã£o

```rust
pub trait PreTokenizer: Send + Sync {
    fn pre_tokenize(&self, text: &str) -> Vec<String>;
}

// Whitespace splitting
pub struct WhitespaceSplit;

// Byte-level (GPT-2): maps bytes to Unicode
pub struct ByteLevel;

// Metaspace (SentencePiece): space -> _
pub struct Metaspace {
    replacement: char,
    add_prefix_space: bool,
}

// Punctuation splitting
pub struct Punctuation;
```

---

### 5. Post-Processamento

```rust
pub trait PostProcessor: Send + Sync {
    fn process(&self, tokens: Vec<u32>, pair: Option<Vec<u32>>) -> Vec<u32>;
}

// BERT: [CLS] + A + [SEP] + B + [SEP]
pub struct BertProcessing {
    cls: u32,
    sep: u32,
}

// RoBERTa: <s> + A + </s> + </s> + B + </s>
pub struct RobertaProcessing {
    bos: u32,
    eos: u32,
}

// Template: custom formatting
pub struct TemplateProcessing {
    template: String, // e.g., "[CLS] $A [SEP] $B [SEP]"
}
```

---

### 6. DecodificaÃ§Ã£o

```rust
pub trait Decoder: Send + Sync {
    fn decode(&self, tokens: &[String]) -> String;
}

// GPT-2 byte-level decoder
pub struct ByteLevelDecoder;

// BERT WordPiece: remove ##
pub struct WordPieceDecoder;

// Metaspace: _ -> space
pub struct MetaspaceDecoder;

// Strip special tokens
pub struct StripDecoder {
    special_tokens: HashSet<String>,
}
```

---

### 7. Treinamento de VocabulÃ¡rio

```rust
pub struct Trainer {
    vocab_size: usize,
    min_frequency: usize,
    special_tokens: Vec<String>,
}

impl Trainer {
    pub fn train_bpe(&self, corpus: &[&str]) -> BPE;
    pub fn train_wordpiece(&self, corpus: &[&str]) -> WordPiece;
    pub fn train_unigram(&self, corpus: &[&str]) -> Unigram;
}
```

**Exemplo:**
```rust
let corpus = vec![
    "OlÃ¡, como vocÃª estÃ¡?",
    "Eu estou bem, obrigado!",
    // ... 1M+ frases
];

let trainer = Trainer::new()
    .vocab_size(50000)
    .min_frequency(2)
    .special_tokens(vec!["[PAD]", "[UNK]", "[CLS]", "[SEP]"]);

let bpe = trainer.train_bpe(&corpus);
bpe.save("vocab.json")?;
```

---

### 8. OtimizaÃ§Ã£o para PortuguÃªs Brasileiro

```rust
pub struct PortugueseTokenizer {
    bpe: BPE,
    normalizer: Sequence,
}

impl PortugueseTokenizer {
    pub fn new() -> Self {
        // Pre-trained no corpus brasileiro (CC-100, Oscar)
        // Vocab otimizado para acentos (Ã¡, Ã©, Ã­, Ã³, Ãº, Ã£, Ãµ, Ã§)
        // Preserva contraÃ§Ãµes (d', l', pr', pra, nÃ©, tÃ¡)
    }
}
```

**Corpus sugerido:**
- BrWaC (2.68B tokens)
- CC-100 Portuguese (71GB)
- Oscar Portuguese (84GB)

---

## ğŸš€ API de Alto NÃ­vel

```rust
// Simple API
let tokenizer = Tokenizer::from_pretrained("gpt2")?;
let ids = tokenizer.encode("Hello, world!")?;
let text = tokenizer.decode(&ids)?;

// Builder API
let tokenizer = Tokenizer::builder()
    .algorithm(Algorithm::BPE)
    .normalizer(NFKC)
    .pre_tokenizer(ByteLevel::default())
    .decoder(ByteLevelDecoder)
    .build()?;

// Batch processing
let texts = vec!["Text 1", "Text 2", "Text 3"];
let encodings = tokenizer.encode_batch(&texts)?;

// With padding/truncation
let encodings = tokenizer.encode_batch(&texts)
    .padding(Padding::MaxLength(512))
    .truncation(Truncation::MaxLength(512))
    .execute()?;
```

---

## ğŸ“Š Performance Targets

### Benchmarks vs Hugging Face Tokenizers

**GPT-2 Tokenization:**
- **HF Tokenizers (Python)**: 100k tokens/sec
- **HF Tokenizers (Rust)**: 1M tokens/sec
- **avila-tokenizers (target)**: **3M tokens/sec** (3x faster)

**BERT Tokenization:**
- **HF**: 500k tokens/sec
- **avila-tokenizers**: **2M tokens/sec** (4x faster)

**Memory:**
- **HF**: ~500MB (loaded model)
- **avila-tokenizers**: **< 100MB** (optimized vocab storage)

---

## ğŸ§ª Testes de Compatibilidade

### Deve passar 100% dos testes:

```rust
#[test]
fn test_gpt2_compatibility() {
    let tokenizer = GPT2Tokenizer::from_pretrained("gpt2").unwrap();

    // Test against known encodings from OpenAI
    assert_eq!(
        tokenizer.encode("Hello, world!"),
        vec![15496, 11, 995, 0] // verified with tiktoken
    );
}

#[test]
fn test_bert_compatibility() {
    let tokenizer = BertTokenizer::from_pretrained("bert-base-uncased").unwrap();

    assert_eq!(
        tokenizer.encode("Hello, world!"),
        vec![101, 7592, 1010, 2088, 999, 102] // [CLS] hello , world ! [SEP]
    );
}

#[test]
fn test_unicode_normalization() {
    let tokenizer = GPT2Tokenizer::from_pretrained("gpt2").unwrap();

    // NFC vs NFD
    let nfc = tokenizer.encode("Ã©"); // U+00E9
    let nfd = tokenizer.encode("Ã©"); // U+0065 U+0301
    assert_eq!(nfc, nfd); // should normalize to same tokens
}

#[test]
fn test_portuguese_accents() {
    let tokenizer = PortugueseTokenizer::new();

    let text = "SÃ£o Paulo, vocÃª estÃ¡ aÃ­?";
    let ids = tokenizer.encode(text);
    let decoded = tokenizer.decode(&ids);
    assert_eq!(text, decoded); // preserve accents
}
```

---

## ğŸ”¬ Casos de Uso AvanÃ§ados

### 1. Streaming Tokenization
```rust
let mut tokenizer_stream = tokenizer.stream();
for chunk in text_stream {
    let tokens = tokenizer_stream.feed(chunk)?;
    process(tokens);
}
let remaining = tokenizer_stream.flush()?;
```

### 2. Custom Vocabulary
```rust
let custom_vocab = vec![
    ("aviladb", 50000),
    ("arxis", 50001),
    ("quaternion", 50002),
];
let tokenizer = tokenizer.extend_vocab(custom_vocab)?;
```

### 3. Token Alignment
```rust
let encoding = tokenizer.encode_with_offsets("Hello, world!")?;
// encoding.tokens: ["Hello", ",", " world", "!"]
// encoding.offsets: [(0, 5), (5, 6), (6, 12), (12, 13)]
```

---

## ğŸ“¦ Cargo.toml

```toml
[package]
name = "avila-tokenizers"
version = "0.1.0"
edition = "2021"
authors = ["NÃ­colas Ãvila <nicolas@avila.inc>"]
license = "MIT OR Apache-2.0"
description = "The most complete tokenizer library in Rust - BPE, WordPiece, Unigram, with native support for GPT, BERT, Llama, Claude"
repository = "https://github.com/avilaops/arxis"
keywords = ["tokenizer", "nlp", "llm", "gpt", "bert"]
categories = ["text-processing", "algorithms"]

[dependencies]
# Zero heavy dependencies - 100% Rust
regex = "1.10"
unicode-normalization = "0.1"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
lru = "0.12"  # LRU cache
rayon = "1.10"  # Parallel processing

[dev-dependencies]
criterion = { version = "0.5", features = ["html_reports"] }
tokio = { version = "1", features = ["rt-multi-thread"] }

[features]
default = []
python = []  # Future: PyO3 bindings
wasm = []    # Future: WASM support

[[bench]]
name = "tokenize_bench"
harness = false
```

---

## ğŸ¯ Deliverables

Ao final, deve entregar:

1. âœ… **CÃ³digo completo** (src/, examples/, tests/, benches/)
2. âœ… **README.md** com exemplos para cada modelo
3. âœ… **Benchmarks** comparando com HF Tokenizers
4. âœ… **Testes** de compatibilidade 100% (GPT-2, BERT, Llama)
5. âœ… **VocabulÃ¡rios** prÃ©-treinados (JSON files)
6. âœ… **DocumentaÃ§Ã£o** (docs.rs quality)
7. âœ… **Performance report** (speedup vs HF)

---

## ğŸ† CritÃ©rios de Sucesso

- [ ] Tokeniza GPT-2 3x mais rÃ¡pido que HF Tokenizers
- [ ] 100% compatÃ­vel com OpenAI tiktoken (GPT-2/3/4)
- [ ] 100% compatÃ­vel com BERT WordPiece
- [ ] Suporta Llama 2/3 SentencePiece
- [ ] Zero dependÃªncias Python
- [ ] < 100MB memory footprint
- [ ] VocabulÃ¡rio portuguÃªs otimizado
- [ ] Testes passam em Windows, Linux, macOS

---

**GO! Construa o tokenizer mais completo do ecossistema Rust! ğŸš€**
