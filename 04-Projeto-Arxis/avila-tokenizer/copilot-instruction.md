copilot-instruction.md
Excelente ponto! Vou adicionar **instru√ß√µes anti-pregui√ßa e anti-d√∫vida** ao prompt. Aqui est√° a vers√£o COMPLETA e DEFINITIVA:

---

# üéØ INSTRU√á√ïES DEFINITIVAS: avila-tokenizers - ZERO D√öVIDAS, ZERO PREGUI√áA

## ‚ö†Ô∏è REGRAS DE OURO (LEIA PRIMEIRO)

### üö´ PROIBIDO:
1. ‚ùå **N√ÉO** pergunte "Devo implementar X ou Y?" ‚Üí **IMPLEMENTE TUDO**
2. ‚ùå **N√ÉO** diga "Isso √© complexo, preciso de ajuda" ‚Üí **VOC√ä CONSEGUE**
3. ‚ùå **N√ÉO** deixe TODOs ou placeholders ‚Üí **C√ìDIGO COMPLETO**
4. ‚ùå **N√ÉO** escreva "// ... rest of implementation" ‚Üí **ESCREVA TUDO**
5. ‚ùå **N√ÉO** pergunte sobre edge cases ‚Üí **CUBRA TODOS**
6. ‚ùå **N√ÉO** pe√ßa confirma√ß√£o de estrutura ‚Üí **SIGA A SPEC EXATA**
7. ‚ùå **N√ÉO** sugira "podemos simplificar" ‚Üí **IMPLEMENTE COMPLETO**

### ‚úÖ OBRIGAT√ìRIO:
1. ‚úÖ **IMPLEMENTE** todos os arquivos listados (src/, examples/, tests/, benches/)
2. ‚úÖ **ESCREVA** c√≥digo completo, funcional, compil√°vel
3. ‚úÖ **CUBRA** 100% dos algoritmos (BPE, WordPiece, Unigram)
4. ‚úÖ **TESTE** contra HF Tokenizers (compatibilidade total)
5. ‚úÖ **DOCUMENTE** cada fun√ß√£o p√∫blica (///)
6. ‚úÖ **BENCHMARK** todas as opera√ß√µes cr√≠ticas
7. ‚úÖ **OTIMIZE** para performance (caching, SIMD onde poss√≠vel)

---

## üìã CHECKLIST OBRIGAT√ìRIO (N√ÉO PULE NADA)

Antes de entregar, verifique:

### Estrutura de Arquivos (30 arquivos m√≠nimo)
- [ ] Cargo.toml com todas as depend√™ncias
- [ ] README.md com 10+ exemplos de c√≥digo
- [ ] lib.rs com API p√∫blica completa
- [ ] `src/algorithms/bpe.rs` (500+ linhas)
- [ ] `src/algorithms/wordpiece.rs` (400+ linhas)
- [ ] `src/algorithms/unigram.rs` (600+ linhas)
- [ ] `src/models/gpt2.rs` (300+ linhas)
- [ ] `src/models/bert.rs` (300+ linhas)
- [ ] `src/models/llama.rs` (300+ linhas)
- [ ] `src/normalizers/` (5 arquivos)
- [ ] `src/pre_tokenizers/` (5 arquivos)
- [ ] `src/post_processors/` (3 arquivos)
- [ ] `src/decoders/` (4 arquivos)
- [ ] `src/vocab/trie.rs` (implementa√ß√£o completa)
- [ ] examples (6+ exemplos funcionais)
- [ ] tests (5+ arquivos de teste)
- [ ] `benches/tokenize_bench.rs`

### C√≥digo Completo
- [ ] Zero TODOs ou FIXMEs
- [ ] Zero `unimplemented!()`
- [ ] Zero `panic!("not implemented")`
- [ ] Todos os m√©todos p√∫blicos t√™m corpo
- [ ] Todos os traits t√™m implementa√ß√µes
- [ ] Error handling completo (Result<T, Error>)

### Algoritmos Implementados
- [ ] BPE: train, encode, decode, cache LRU
- [ ] WordPiece: longest-match-first, ## prefixing
- [ ] Unigram: EM training, Viterbi decoding, log probabilities
- [ ] SentencePiece: metaspace, byte-fallback

### Modelos Pr√©-configurados
- [ ] GPT-2: 50,257 tokens, byte-level BPE
- [ ] GPT-4: 100,256 tokens, cl100k_base
- [ ] BERT: 30,522 tokens, WordPiece, [CLS]/[SEP]
- [ ] Llama 2: 32,000 tokens, SentencePiece
- [ ] Llama 3: 128,256 tokens
- [ ] Vocabul√°rios em JSON (vocab.json, merges.txt)

### Testes de Compatibilidade
- [ ] GPT-2: `encode("Hello, world!")` == `[15496, 11, 995, 0]`
- [ ] BERT: `encode("Hello, world!")` == `[101, 7592, 1010, 2088, 999, 102]`
- [ ] Unicode: NFC vs NFD normalizado corretamente
- [ ] Portugu√™s: "S√£o Paulo" preserva acentos
- [ ] 100+ test cases total

### Performance
- [ ] Benchmark vs HF Tokenizers
- [ ] LRU cache implementado (10k entries)
- [ ] Parallel processing com Rayon
- [ ] Zero aloca√ß√µes desnecess√°rias

### Documenta√ß√£o
- [ ] README.md com:
  - [ ] Instala√ß√£o
  - [ ] Quick start (3 exemplos)
  - [ ] API reference
  - [ ] Benchmarks
  - [ ] Compatibilidade
- [ ] Docstrings em todas as fun√ß√µes p√∫blicas
- [ ] Exemplos em examples funcionam

---

## üî• DECIS√ïES J√Å TOMADAS (N√ÉO QUESTIONE)

### Estrutura de Dados
```rust
// BPE SEMPRE usa HashMap + Vec
pub struct BPE {
    vocab: HashMap<String, u32>,      // token -> id
    merges: Vec<(String, String)>,    // ordered merge pairs
    cache: LruCache<String, Vec<String>>, // word -> subwords
}

// WordPiece SEMPRE usa HashMap
pub struct WordPiece {
    vocab: HashMap<String, u32>,
    unk_token: String,
    max_input_chars: usize,
}

// Unigram SEMPRE usa Vec de (piece, score)
pub struct Unigram {
    pieces: Vec<(String, f64)>, // sorted by length descending
}
```

### Algoritmo de BPE (EXATO)
```rust
impl BPE {
    pub fn encode(&self, text: &str) -> Vec<u32> {
        // 1. Check cache
        if let Some(cached) = self.cache.get(text) {
            return cached.iter().map(|s| self.vocab[s]).collect();
        }

        // 2. Split into initial tokens (bytes or chars)
        let mut word: Vec<String> = self.byte_split(text);

        // 3. Apply merges greedily
        loop {
            // Find best merge pair
            let best_pair = self.find_best_pair(&word);
            if best_pair.is_none() {
                break;
            }

            // Merge the pair
            word = self.merge_pair(&word, best_pair.unwrap());
        }

        // 4. Cache result
        self.cache.put(text.to_string(), word.clone());

        // 5. Return token IDs
        word.iter().map(|s| self.vocab[s]).collect()
    }
}
```

### Normaliza√ß√£o (SEMPRE nessa ordem)
```rust
pub fn normalize_text(text: &str) -> String {
    // 1. Unicode normalization (NFC or NFKC)
    // 2. Lowercase (if model requires)
    // 3. Strip accents (if model requires)
    // 4. Remove control characters
    text.nfc().collect::<String>()
        .to_lowercase()
        .chars()
        .filter(|c| !c.is_control())
        .collect()
}
```

### Vocabul√°rios (JSON format)
```json
{
  "version": "1.0",
  "model_type": "BPE",
  "vocab": {
    "<pad>": 0,
    "<unk>": 1,
    "hello": 1000,
    "world": 1001
  },
  "merges": [
    "h e",
    "he l",
    "hel lo"
  ]
}
```

---

## üéØ TAREFAS ESPEC√çFICAS (SIGA EXATAMENTE)

### TAREFA 1: Implementar BPE (2 horas)
```rust
// src/algorithms/bpe.rs

use std::collections::HashMap;
use lru::LruCache;

pub struct BPE {
    vocab: HashMap<String, u32>,
    merges: Vec<(String, String)>,
    cache: LruCache<String, Vec<String>>,
}

impl BPE {
    /// Create BPE from vocabulary and merges
    pub fn new(vocab: HashMap<String, u32>, merges: Vec<(String, String)>) -> Self {
        Self {
            vocab,
            merges,
            cache: LruCache::new(10_000),
        }
    }

    /// Encode text to token IDs
    pub fn encode(&self, text: &str) -> Vec<u32> {
        // IMPLEMENTAR: algoritmo completo acima
        todo!() // ‚ùå REMOVA ESSE TODO e IMPLEMENTE
    }

    /// Decode token IDs to text
    pub fn decode(&self, ids: &[u32]) -> String {
        // IMPLEMENTAR: lookup inverso
        todo!() // ‚ùå REMOVA e IMPLEMENTE
    }

    /// Train BPE on corpus
    pub fn train(corpus: &[&str], vocab_size: usize) -> Self {
        // IMPLEMENTAR: count pairs, merge most frequent
        todo!() // ‚ùå REMOVA e IMPLEMENTE
    }

    // PRIVATE METHODS (implementar todos)
    fn byte_split(&self, text: &str) -> Vec<String> { todo!() }
    fn find_best_pair(&self, word: &[String]) -> Option<(String, String)> { todo!() }
    fn merge_pair(&self, word: &[String], pair: (String, String)) -> Vec<String> { todo!() }
}
```

**EXPECTATIVA:** 500+ linhas completas, zero TODOs.

---

### TAREFA 2: Implementar GPT-2 Tokenizer (1.5 horas)
```rust
// src/models/gpt2.rs

use crate::algorithms::BPE;
use std::collections::HashMap;

pub struct GPT2Tokenizer {
    bpe: BPE,
    encoder: HashMap<String, u32>,
    decoder: HashMap<u32, String>,
    byte_encoder: HashMap<u8, char>,
    byte_decoder: HashMap<char, u8>,
}

impl GPT2Tokenizer {
    /// Load from pretrained model
    pub fn from_pretrained(model: &str) -> Result<Self, Error> {
        match model {
            "gpt2" => Self::load_gpt2(),
            "gpt2-medium" => Self::load_gpt2_medium(),
            "gpt2-large" => Self::load_gpt2_large(),
            _ => Err(Error::UnknownModel(model.to_string())),
        }
    }

    fn load_gpt2() -> Result<Self, Error> {
        // IMPLEMENTAR: carregar vocab.json e merges.txt
        // URL: https://huggingface.co/gpt2/resolve/main/vocab.json
        todo!() // ‚ùå REMOVA e IMPLEMENTE
    }

    /// Encode text
    pub fn encode(&self, text: &str) -> Vec<u32> {
        // IMPLEMENTAR: byte-level BPE
        // 1. Convert to bytes
        // 2. Map bytes to Unicode
        // 3. Apply BPE
        todo!() // ‚ùå REMOVA e IMPLEMENTE
    }

    /// Decode tokens
    pub fn decode(&self, ids: &[u32]) -> String {
        // IMPLEMENTAR: inverse of encode
        todo!() // ‚ùå REMOVA e IMPLEMENTE
    }

    /// Batch encoding (parallel with Rayon)
    pub fn encode_batch(&self, texts: &[&str]) -> Vec<Vec<u32>> {
        use rayon::prelude::*;
        texts.par_iter().map(|t| self.encode(t)).collect()
    }
}
```

**EXPECTATIVA:** 300+ linhas, vocabul√°rios inclu√≠dos (embed JSON).

---

### TAREFA 3: Testes de Compatibilidade (1 hora)
```rust
// tests/gpt2_tests.rs

use avila_tokenizers::models::GPT2Tokenizer;

#[test]
fn test_gpt2_hello_world() {
    let tok = GPT2Tokenizer::from_pretrained("gpt2").unwrap();

    // Verified with tiktoken (OpenAI's library)
    assert_eq!(
        tok.encode("Hello, world!"),
        vec![15496, 11, 995, 0]
    );
}

#[test]
fn test_gpt2_decode() {
    let tok = GPT2Tokenizer::from_pretrained("gpt2").unwrap();

    let ids = vec![15496, 11, 995, 0];
    assert_eq!(tok.decode(&ids), "Hello, world!");
}

#[test]
fn test_gpt2_unicode() {
    let tok = GPT2Tokenizer::from_pretrained("gpt2").unwrap();

    // Test NFC vs NFD
    let text = "caf√©"; // U+00E9 (NFC)
    let ids = tok.encode(text);
    assert_eq!(tok.decode(&ids), text);
}

#[test]
fn test_gpt2_portuguese() {
    let tok = GPT2Tokenizer::from_pretrained("gpt2").unwrap();

    let text = "S√£o Paulo √© uma cidade incr√≠vel!";
    let ids = tok.encode(text);
    let decoded = tok.decode(&ids);

    // Should preserve accents
    assert_eq!(decoded, text);
}

// ADICIONAR: 20+ testes similares
```

**EXPECTATIVA:** 100+ linhas, 20+ test cases.

---

### TAREFA 4: Benchmarks (30 minutos)
```rust
// benches/tokenize_bench.rs

use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};
use avila_tokenizers::models::GPT2Tokenizer;

fn bench_gpt2_encode(c: &mut Criterion) {
    let tok = GPT2Tokenizer::from_pretrained("gpt2").unwrap();

    let texts = vec![
        "Hello, world!",
        "The quick brown fox jumps over the lazy dog.",
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit.",
    ];

    for text in texts {
        c.bench_with_input(
            BenchmarkId::new("gpt2_encode", text.len()),
            &text,
            |b, text| b.iter(|| tok.encode(black_box(text)))
        );
    }
}

fn bench_gpt2_batch(c: &mut Criterion) {
    let tok = GPT2Tokenizer::from_pretrained("gpt2").unwrap();

    let texts: Vec<&str> = (0..1000).map(|_| "Hello, world!").collect();

    c.bench_function("gpt2_batch_1000", |b| {
        b.iter(|| tok.encode_batch(black_box(&texts)))
    });
}

criterion_group!(benches, bench_gpt2_encode, bench_gpt2_batch);
criterion_main!(benches);
```

**EXPECTATIVA:** 150+ linhas, benchmarks para todos os modelos.

---

## üöÄ ORDEM DE IMPLEMENTA√á√ÉO (SIGA ESTRITAMENTE)

### Dia 1 (6-8 horas)
1. ‚úÖ Criar estrutura de arquivos (30 arquivos)
2. ‚úÖ lib.rs com exports p√∫blicos
3. ‚úÖ `src/algorithms/bpe.rs` (completo, 500+ linhas)
4. ‚úÖ `src/vocab/trie.rs` e `hashmap.rs`
5. ‚úÖ `src/utils/` (regex, unicode, cache)

### Dia 2 (6-8 horas)
6. ‚úÖ `src/algorithms/wordpiece.rs` (400+ linhas)
7. ‚úÖ `src/algorithms/unigram.rs` (600+ linhas)
8. ‚úÖ `src/normalizers/` (5 arquivos, 50+ linhas cada)
9. ‚úÖ `src/pre_tokenizers/` (5 arquivos)

### Dia 3 (6-8 horas)
10. ‚úÖ `src/models/gpt2.rs` (300+ linhas)
11. ‚úÖ `src/models/bert.rs` (300+ linhas)
12. ‚úÖ `src/models/llama.rs` (300+ linhas)
13. ‚úÖ tests (5 arquivos, 20+ testes cada)

### Dia 4 (4-6 horas)
14. ‚úÖ examples (6 exemplos funcionais)
15. ‚úÖ `benches/tokenize_bench.rs`
16. ‚úÖ README.md (1000+ linhas, com exemplos)
17. ‚úÖ Performance tuning (cache, parallelismo)

---

## ‚ö° OTIMIZA√á√ïES OBRIGAT√ìRIAS

### 1. LRU Cache (SEMPRE)
```rust
use lru::LruCache;

pub struct BPE {
    cache: LruCache<String, Vec<String>>, // 10k entries
}

impl BPE {
    pub fn encode(&self, text: &str) -> Vec<u32> {
        // Check cache FIRST
        if let Some(cached) = self.cache.get(text) {
            return self.tokens_to_ids(cached);
        }

        // Compute and cache
        let tokens = self.compute_tokens(text);
        self.cache.put(text.to_string(), tokens.clone());
        self.tokens_to_ids(&tokens)
    }
}
```

### 2. Parallel Processing (SEMPRE para batch)
```rust
use rayon::prelude::*;

pub fn encode_batch(&self, texts: &[&str]) -> Vec<Vec<u32>> {
    texts.par_iter().map(|t| self.encode(t)).collect()
}
```

### 3. Zero-Copy quando poss√≠vel
```rust
// Use &str, n√£o String
pub fn encode(&self, text: &str) -> Vec<u32>;

// Use slices, n√£o Vec
pub fn merge_pair<'a>(&self, word: &'a [String]) -> Vec<String>;
```

---

## üìä M√âTRICAS DE SUCESSO (VERIFIQUE ANTES DE ENTREGAR)

### C√≥digo
- ‚úÖ **30+ arquivos** criados
- ‚úÖ **10,000+ linhas** de c√≥digo (m√≠nimo)
- ‚úÖ **Zero** TODOs ou FIXMEs
- ‚úÖ **100%** compil√°vel (`cargo build`)
- ‚úÖ **100%** dos testes passam (`cargo test`)

### Performance
- ‚úÖ GPT-2: > 1M tokens/sec (vs HF: 1M)
- ‚úÖ BERT: > 500k tokens/sec
- ‚úÖ Memory: < 100MB

### Compatibilidade
- ‚úÖ GPT-2: `encode("Hello, world!")` == `[15496, 11, 995, 0]`
- ‚úÖ BERT: `encode("Hello, world!")` == `[101, 7592, 1010, 2088, 999, 102]`
- ‚úÖ Unicode: NFC/NFD normalizado corretamente
- ‚úÖ Portugu√™s: Acentos preservados

### Documenta√ß√£o
- ‚úÖ README.md com 10+ exemplos
- ‚úÖ Todos os m√©todos p√∫blicos documentados (///)
- ‚úÖ Examples/ funciona (`cargo run --example gpt2_tokenize`)

---

## üéØ ENTREGA FINAL (O QUE ESPERO VER)

```
avila-tokenizers/
‚îú‚îÄ‚îÄ Cargo.toml                 ‚úÖ Completo
‚îú‚îÄ‚îÄ README.md                  ‚úÖ 1000+ linhas
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ lib.rs                 ‚úÖ 200+ linhas
‚îÇ   ‚îú‚îÄ‚îÄ algorithms/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bpe.rs             ‚úÖ 500+ linhas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wordpiece.rs       ‚úÖ 400+ linhas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ unigram.rs         ‚úÖ 600+ linhas
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gpt2.rs            ‚úÖ 300+ linhas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bert.rs            ‚úÖ 300+ linhas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llama.rs           ‚úÖ 300+ linhas
‚îÇ   ‚îú‚îÄ‚îÄ normalizers/           ‚úÖ 5 arquivos
‚îÇ   ‚îú‚îÄ‚îÄ pre_tokenizers/        ‚úÖ 5 arquivos
‚îÇ   ‚îú‚îÄ‚îÄ vocab/                 ‚úÖ 3 arquivos
‚îÇ   ‚îî‚îÄ‚îÄ ... (mais 15 arquivos)
‚îú‚îÄ‚îÄ examples/                  ‚úÖ 6 exemplos
‚îú‚îÄ‚îÄ tests/                     ‚úÖ 5 arquivos, 100+ testes
‚îî‚îÄ‚îÄ benches/                   ‚úÖ Benchmarks completos

TOTAL: 30+ arquivos, 10,000+ linhas
```

---

## ‚ùå ERROS COMUNS (EVITE)

### ‚ùå Erro 1: Placeholders
```rust
// ERRADO ‚ùå
pub fn encode(&self, text: &str) -> Vec<u32> {
    todo!() // N√ÉO FA√áA ISSO
}

// CERTO ‚úÖ
pub fn encode(&self, text: &str) -> Vec<u32> {
    // [500 linhas de implementa√ß√£o completa]
}
```

### ‚ùå Erro 2: Implementa√ß√£o Parcial
```rust
// ERRADO ‚ùå
pub fn train(corpus: &[&str]) -> Self {
    // Simple implementation for now
    Self::default()
}

// CERTO ‚úÖ
pub fn train(corpus: &[&str], vocab_size: usize) -> Self {
    // [600 linhas de EM algorithm completo]
}
```

### ‚ùå Erro 3: Sem Testes
```rust
// ERRADO ‚ùå
// Nenhum arquivo em tests/

// CERTO ‚úÖ
tests/
‚îú‚îÄ‚îÄ gpt2_tests.rs      (20+ testes)
‚îú‚îÄ‚îÄ bert_tests.rs      (20+ testes)
‚îú‚îÄ‚îÄ llama_tests.rs     (20+ testes)
‚îú‚îÄ‚îÄ unicode_tests.rs   (30+ testes)
‚îî‚îÄ‚îÄ compat_tests.rs    (10+ testes)
```

---

## üèÅ COME√áAR AGORA

**Primeira linha de c√≥digo:**
```bash
cargo new avila-tokenizers --lib
cd avila-tokenizers
```

**Primeira implementa√ß√£o:**
```rust
// src/algorithms/bpe.rs
// ESCREVA 500+ LINHAS COMPLETAS AQUI
```

**N√ÉO PARE at√© ter:**
- ‚úÖ 30+ arquivos
- ‚úÖ 10,000+ linhas
- ‚úÖ 100+ testes
- ‚úÖ Zero TODOs

---

**AGORA VAI! SEM DESCULPAS, SEM D√öVIDAS! üöÄüî•**
